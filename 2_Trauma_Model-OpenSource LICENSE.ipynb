{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Copyright 2020 Parkland Health & Hospital System </center>\n",
    "\n",
    "This program entitled “Parkland Trauma Index of Mortality” is free software and is distributed under the terms of the GNU Lesser General Public License (LGPL). You can redistribute it and/or modify it under the terms of the GNU LGPL as published by the Free Software Foundation, either version 3 of the License or any later version. This program is distributed WITHOUT ANY WARRANTY; without even THE IMPLIED WARRANTY OF MERCHANTABILITY or FITTNESS FOR A PARTICULAR PURPOSE. See the GNU LGPL for more details. You should have received a copy of the GNU LGPL along with this program; if not, see https://www.gnu.org/licenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dependencies\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import copy, sys\n",
    "import numpy as np\n",
    "import sqlite3 as lite\n",
    "import collections\n",
    "import scikitplot\n",
    "import sklearn # 0.21.3  Balanced bagging classifier from imblearn needs this version of sklearn for sklearn.externals.joblib\n",
    "import six\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from  scikitplot.metrics import plot_confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split,  GroupKFold, StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scikitplot import classifier_factory\n",
    "from sklearn.metrics import roc_curve,roc_auc_score, precision_score, recall_score, f1_score,classification_report,confusion_matrix, accuracy_score, precision_recall_curve\n",
    "import joblib\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_rows = 100\n",
    "pd.set_option('display.max_columns', 500)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "## Set number of hours to use when resampling clinical data\n",
    "TIME_BINS = '12H'\n",
    "\n",
    "## Set maximum number of time periods to include per encounter\n",
    "MAX_PERIODS = 6\n",
    "TRUNCATE_LONG_STAYS = 1\n",
    "IMPUTE = 0\n",
    "\n",
    "## Set minimum number of hours admitted before prediction\n",
    "MIN_LOS = 12\n",
    "\n",
    "## Set number of jobs to use in parallel (-1 = max)\n",
    "N_JOBS = -1\n",
    "\n",
    "## Set base path for raw data\n",
    "DATA_PATH = 'path to data file'\n",
    "\n",
    "## Set default SQLite database file; default will be in same path as raw data files\n",
    "SQLITE_DB = DATA_PATH + \"trauma_mortality.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write selected dataframe to default SQLite database, replacing if found\n",
    "def DataFrameToSQL(df, df_name):\n",
    "    connection = lite.connect(SQLITE_DB)\n",
    "    with connection:\n",
    "        df.to_sql(df_name, connection, schema = None, if_exists='replace', index=False)\n",
    "        print('%s backed up to default SQLite database.' % df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve SQL table from default SQLite database and store as a dataframe\n",
    "def SQLToDataFrame(table_name, date_columns=[]):\n",
    "    connection = lite.connect(SQLITE_DB)\n",
    "    sql = 'select * from ' + table_name\n",
    "    df = pd.read_sql(sql, connection, parse_dates=date_columns)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieves raw data for any patient who either has a ta (OR) st with all of the features defined or built in data preparation file.\n",
    "raw_data = SQLToDataFrame(table_name='merge_all', date_columns=['date_of_arrival', 'current_datetime', 'current_datetime_min', 'birthdate', 'deathdate', 'hosp_admsn_time', 'hosp_dischrg_time','adm_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a copy of raw data before making any changes since raw data can take some time to be imported into python.\n",
    "# Main Cohort: data\n",
    "data = raw_data.copy()\n",
    "data['time_elapsed'] = data.time_sequence * np.timedelta64(12, 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data of a patient with any of the ta levels mentioned\n",
    "ta_level123 =  pd.read_csv(DATA_PATH + 'Trauma_Cohort_L123Only.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude any patient under 18\n",
    "data = data[data.age>= 18.0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve level 1 ta and merge it with labs, vitals, etc., from data. # TRAUMA ACTIVATION LEVEL 1 only\n",
    "ta_level1= ta_level123[ta_level123.TraumaTypeMerged.isin(['Level 1'])]\n",
    "ta1_data = pd.merge(ta_level1,data, how = 'left', left_on = 'EncounterEpicCsn', right_on = 'pat_enc_csn_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COPY the above retrieved dataframes to 'data' below. Just to be safe about not messing the above cohort and any code below.\n",
    "#data = ta_and_st1.copy()\n",
    "data = ta1_data.copy()\n",
    "data.pat_enc_csn_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop rows with missing dependent variable\n",
    "data.dropna(axis=0, how='any', thresh=None, subset=['death_flag_next_period'], inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## two periods has to include next period\n",
    "data['death_flag_two_periods'] = (data['death_flag_two_periods'] + data['death_flag_next_period'] >= 1.0).astype(int)\n",
    "data['death_flag_three_periods'] = (data['death_flag_three_periods'] + data['death_flag_two_periods'] >= 1.0).astype(int)\n",
    "data['death_flag_four_periods'] = (data['death_flag_four_periods'] + data['death_flag_three_periods'] >= 1.0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Truncate long tail of hospital stays using MAX_PERIODS global variable\n",
    "if TRUNCATE_LONG_STAYS == 1:\n",
    "    data = data.set_index(['primarymrn', 'pat_enc_csn_id', 'time_sequence']).drop(list(range(MAX_PERIODS+1, 1000)), level='time_sequence').reset_index()\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop last observation\n",
    "data = data.loc[data.death_flag_this_period != 1]\n",
    "print(data.shape)\n",
    "\n",
    "## Remove stays shorter than 12 hours; should already be fixed by the above\n",
    "data = data[data.groupby(['primarymrn','pat_enc_csn_id'])['time_sequence'].transform(max) >= int(MIN_LOS / 12)]\n",
    "print(data.shape)\n",
    "\n",
    "## Include only time_sequence > MIN_LOS\n",
    "data = data[data.time_sequence >= int(MIN_LOS/12)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad cases where only 1 time sequence is available\n",
    "#data = data[~((data.time_sequence == data.groupby(['pat_enc_csn_id']).time_sequence.transform(max)) & (data.time_sequence== 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_column =  'death_flag_four_periods' # Number of 12 hour intervals to be predicted (4 = 48 hr prediction time)\n",
    "\n",
    "X_columns = ['pat_enc_csn_id', \n",
    "             'time_sequence', 'age', \n",
    "             'temperature_max', 'pkmod_r_cpn_glasgow_coma_scale_score',\n",
    "             'hemoglobin_min', 'potassium_min', 'potassium_max',\n",
    "             'pulse_max', 'wbc_max', 'lactate_max', 'inr_max', 'creatinine_max',\n",
    "             'ast_max',  'bilirubin_total_max', 'systolic_max',\n",
    "             'pulse_min', 'pulse_oximetry_min',\n",
    "             'platelets_min', 'base_exc_art_min', 'albumin_min', 'systolic_min',\n",
    "             'pulse_avg', 'pulse_oximetry_avg', \n",
    "            'arrival_year'\n",
    "             ]\n",
    "features_cat = ['time_sequence'] \n",
    "features_cont_float = [\n",
    "             'temperature_max', \n",
    "             'hemoglobin_min', 'potassium_min', 'potassium_max',\n",
    "             'pulse_max', 'wbc_max', 'lactate_max', 'inr_max', 'creatinine_max',\n",
    "             'ast_max',  'bilirubin_total_max', 'systolic_max',\n",
    "             'pulse_min', 'pulse_oximetry_min',\n",
    "             'platelets_min', 'base_exc_art_min', 'albumin_min', 'systolic_min',\n",
    "             'pulse_avg', 'pulse_oximetry_avg']\n",
    "\n",
    "features_cont_int = ['age','pkmod_r_cpn_glasgow_coma_scale_score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if y_column == 'death_flag_four_periods':\n",
    "    data[data['death_flag_next_period'] == 1]['death_flag_four_periods'] = 1\n",
    "    data[data['death_flag_two_periods'] == 1]['death_flag_four_periods'] = 1\n",
    "    data[data['death_flag_three_periods'] == 1]['death_flag_four_periods'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = data.copy().reset_index()\n",
    "\n",
    "features = X_columns\n",
    "X = data_[X_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPUTE == 1:\n",
    "    imp = SimpleImputer(missing_values = 'NaN' , strategy = 'median', axis=0, copy=True)\n",
    "    X = imp.fit_transform(X)    \n",
    "    \n",
    "else:\n",
    "    X = X.fillna(method = 'pad', limit = 2)\n",
    "    X = X.fillna(-9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y = data_[[y_column, 'arrival_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.pat_enc_csn_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 7\n",
    "K_FOLDS = 3\n",
    "TEST_SIZE = .25\n",
    "SEED = 13\n",
    "\n",
    "BALANCE_RATIO = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_resampled = X.loc[X.arrival_year.isin([2009, 2010, 2011, 2012, 2013,2014])]\n",
    "X_test_holdout = X.loc[X.arrival_year.isin([2015, 2016])]\n",
    "Y_train_resampled = Y.loc[Y.arrival_year.isin([2009, 2010, 2011, 2012, 2013,2014])][y_column]\n",
    "Y_test_holdout = Y.loc[Y.arrival_year.isin([2015, 2016])][y_column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_holdout.pat_enc_csn_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sd = X_train_resampled.drop(['pat_enc_csn_id', 'arrival_year'], axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_names = X_train_resampled.drop(['pat_enc_csn_id', 'arrival_year'], axis = 1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TA - Level 1 Only as cohort\n",
    "# BalancedBaggingClassifier(base_estimator=None, bootstrap=True,\n",
    "#              bootstrap_features=False, max_features=1.0, max_samples=0.6,\n",
    "#              n_estimators=200, n_jobs=7, oob_score=True, random_state=13,\n",
    "#              ratio=0.994436655762512, replacement=True, verbose=False,\n",
    "#              warm_start=False) \n",
    "#ratio=0.9920599001135434\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [16,9]\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rcParams.update({'axes.labelsize': 'medium'})\n",
    "\n",
    "try:\n",
    "    features.remove('pat_enc_csn_id')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "N_ESTIMATORS = 175 #700\n",
    "MAX_DEPTH = 8 #4\n",
    "MIN_SAMPLES_SPLIT = 10\n",
    "MIN_SAMPLES_LEAF = 4 #4\n",
    "MAX_FEATURES = 1.0 \n",
    "CRITERION = 'entropy' \n",
    "WARM_START = False\n",
    "CLASS_WEIGHT = {1: RATIO}  \n",
    "\n",
    "model_list = [\n",
    "BalancedBaggingClassifier(base_estimator=None, bootstrap=True,\n",
    "             bootstrap_features=False, max_features=1.0, max_samples=0.6,\n",
    "             n_estimators=200, n_jobs=7, oob_score=True, random_state=13,\n",
    "             ratio=0.994436655762512, replacement=True, verbose=False,\n",
    "             warm_start=False)\n",
    "]       \n",
    "\n",
    "for model in model_list:\n",
    "    rfr = model\n",
    "    rfr.fit(X_train_resampled.drop(['pat_enc_csn_id','arrival_year'],axis = 1), Y_train_resampled)\n",
    "    \n",
    "    ##Save to pickle file\n",
    "    #joblib.dump(rfr,'trauma.pkl')\n",
    "    \n",
    "    print(y_column)\n",
    "    print (\"Model: %s \" % str(rfr).split('(')[0])\n",
    "    print (\"Accuracy Score (Test): %3.3f\" % np.round(rfr.score(X_test_holdout.drop(['pat_enc_csn_id','arrival_year'],axis = 1), Y_test_holdout), 3))\n",
    "\n",
    "    try:\n",
    "        importances = pd.DataFrame({'feature':X_columns,\n",
    "            'importance':np.round(rfr.feature_importances_,3)})\n",
    "        importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "        print (importances[:5])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    predicted = rfr.predict(X_test_holdout.drop(['pat_enc_csn_id','arrival_year'],axis = 1))\n",
    "    print(sklearn.metrics.classification_report(Y_test_holdout, predicted))\n",
    "\n",
    "    rfr2=copy.deepcopy(rfr)\n",
    "    Y_probas = rfr2.predict_proba(X_test_holdout.drop(['pat_enc_csn_id','arrival_year'],axis = 1).values) #as_matrix())#[:,1]\n",
    "\n",
    "    print(sklearn.metrics.roc_auc_score(Y_test_holdout, Y_probas[:,1]))\n",
    "    \n",
    "    scikitplot.metrics.plot_roc(Y_test_holdout, Y_probas, plot_micro=False, plot_macro = False, classes_to_plot = [1])\n",
    "    plt.legend(['AUC: 0.94'], loc = 'lower right' )\n",
    "    plt.savefig('ROC_Curve_BBC.jpg', transparent = True, bbox_inches = 'tight')\n",
    "    \n",
    "    scikitplot.metrics.plot_precision_recall(Y_test_holdout, Y_probas, plot_micro = False,classes_to_plot=[1])\n",
    "    plt.legend(['Area: 0.377'], loc = 'lower right')\n",
    "    plt.savefig('PR_Curve_BBC.jpg', transparent = True, bbox_inches = 'tight')\n",
    "    \n",
    "    scikitplot.metrics.plot_cumulative_gain(Y_test_holdout, Y_probas)\n",
    "    plt.savefig('GainChart_BBC.jpg', transparent = True, bbox_inches = 'tight')\n",
    "    \n",
    "    scikitplot.metrics.plot_lift_curve(Y_test_holdout, Y_probas)\n",
    "    plt.savefig('Lift_Curve_BBC.jpg', transparent = True, bbox_inches = 'tight')\n",
    "    \n",
    "    scikitplot.metrics.plot_confusion_matrix(Y_test_holdout, predicted, normalize = False)\n",
    "    plt.savefig('ConfusionMatrix_BBC.jpg', transparent = True, bbox_inches = 'tight')\n",
    "    xt = X_train_resampled.drop(['pat_enc_csn_id','arrival_year'],axis = 1).copy()\n",
    "    \n",
    "    \n",
    "    #scikitplot.estimators.plot_feature_importances(rfr2,X_columns)\n",
    "    plt.show()\n",
    "    Y_test_preds = pd.DataFrame(Y_test_holdout.copy())\n",
    "    Y_test_preds['Predictions'] = predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sd = X_test_holdout.drop(['pat_enc_csn_id', 'arrival_year'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "for i, j in zip(np.round(np.mean([est.steps[1][1].feature_importances_ for est in rfr.estimators_], axis=0), 3), features):\n",
    "    feature_dict[j] = i\n",
    "\n",
    "feat_imp = pd.DataFrame.from_dict(feature_dict, orient='index').sort_values(0, ascending=False)#.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = feat_imp.reset_index()\n",
    "feat_imp.columns = ['feature','importance']\n",
    "feat_imp = feat_imp.replace(['temperature_max','age','time_sequence','pkmod_r_cpn_glasgow_coma_scale_score','hemoglobin_min','potassium_min','potassium_max','pulse_max','wbc_max','lactate_max','inr_max','creatinine_max','ast_max', 'bilirubin_total_max','systolic_max','pulse_min','pulse_oximetry_min','platelets_min','base_exc_art_min','albumin_min','systolic_min','pulse_avg','pulse_oximetry_avg'],\n",
    "                 ['Maximum Temperature','Age','Time Since Arrival','Glasgow Coma Scale','Minimum Hemoglobin','Minimum Potassium','Maximum Potassium','Maximum Pulse','Maximum WBC','Maximum Lactate','Maximum INR','Maximum Creatinine', 'Maximum AST', 'Maximum Total Bilirubin','Maximum Systolic','Minimum Pulse','Minimum Pulse Oximetry', 'Minimum Platelets','Minimum Base Deficit', 'Minimum Albumin','Minimum Systolic','Average Pulse','Average Pulse Oximetry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING FEATURE IMPORTANCE\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 9)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.grid(axis = 'x', linewidth = 0.2)\n",
    "plt.barh(feat_imp.sort_values('importance')['feature'],feat_imp.sort_values('importance')['importance'], color = 'black')\n",
    "plt.savefig('Feature_importance_bbc_manuscript.jpg', transparent = True, bbox_inches = 'tight')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
