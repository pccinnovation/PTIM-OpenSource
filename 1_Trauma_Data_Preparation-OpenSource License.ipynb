{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Copyright 2020 Parkland Health & Hospital System </center>\n",
    "\n",
    "This program entitled “Parkland Trauma Index of Mortality” is free software and is distributed under the terms of the GNU Lesser General Public License (LGPL). You can redistribute it and/or modify it under the terms of the GNU LGPL as published by the Free Software Foundation, either version 3 of the License or any later version. This program is distributed WITHOUT ANY WARRANTY; without even THE IMPLIED WARRANTY OF MERCHANTABILITY or FITTNESS FOR A PARTICULAR PURPOSE. See the GNU LGPL for more details. You should have received a copy of the GNU LGPL along with this program; if not, see https://www.gnu.org/licenses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "- Point global variables to raw data path. \n",
    "- 1_Trauma_Data_Preparation prepares the data for analysis\n",
    "- 2_Trauma_Model applies some final cleaning steps and fits models\n",
    "\n",
    "\n",
    "________________________________________________________\n",
    "## Methodology \n",
    "**Out-of-ICU model validation:** Generate misclassification table, ROC curve, C-statistic\n",
    "\n",
    "**Predict mortality:** Classification model comparison\n",
    "\n",
    "**Unit of analysis:** [12] hour groupings of an inpatient stay associated with a trauma activation, with optional resampling to test for bias, and optimize time-to-intervene.\n",
    "\n",
    "**Outcome:** mortality within hospital stay\n",
    "________________________________________________________\n",
    "## Dependencies \n",
    "### Data Sources and Connections\n",
    "- Clinical + Demographics: Connection to Parkland's Clarity and EDW data via Composite ODBC\n",
    "\n",
    "### Software\n",
    "- Python 3.6 \n",
    "- SQLite\n",
    "\n",
    "### Libraries\n",
    "- Pandas\n",
    "- NumPy\n",
    "- CSV\n",
    "- SQLite3\n",
    "- Matplotlib\n",
    "- Seaborn\n",
    "- FancyImpute (mice.py - local import)\n",
    "- MissingNo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dependencies\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import sqlite3 as lite\n",
    "import missingno as mn\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "## Set number of hours to use when resampling clinical data\n",
    "TIME_BINS = '12H'\n",
    "\n",
    "## Set maximum number of time periods to include per encounter\n",
    "MAX_PERIODS = 60\n",
    "\n",
    "## Set number of jobs to use in parallel (-1 = max)\n",
    "N_JOBS = 3\n",
    "\n",
    "## Set base path for raw data\n",
    "\n",
    "DATA_PATH = 'path to data file'\n",
    "\n",
    "\n",
    "## Set default SQLite database file; default will be in same path as raw data files\n",
    "SQLITE_DB = DATA_PATH + \"trauma_mortality_backup_tast.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace spaces in column names (caused by IDT/WebBI) with underscores; make all headers lowercase\n",
    "### Inputs: list of dataframes, list of dataframe names\n",
    "### Outputs: None. Makes changes in place.\n",
    "def ColumnSpaceToUnderscore(all_data, file_list):   \n",
    "    i = 0\n",
    "    while i < len(file_list):\n",
    "        column_dict = {}\n",
    "        for j in range(0, len(all_data[file_list[i]].columns.values)):\n",
    "            column_dict[all_data[file_list[i]].columns.values[j]] = all_data[file_list[i]].columns.values[j].replace(' ', '_')\n",
    "        all_data[file_list[i]].rename(columns=column_dict, inplace=True)\n",
    "        all_data[file_list[i]].rename(columns=str.lower, inplace=True)\n",
    "        i += 1\n",
    "    print('Column names are now lower-case and free of spaces.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm correct datatypes coming across in all input fields\n",
    "def CheckDataTypes(list_of_dataframes, file_list):\n",
    "    i = 0\n",
    "    while i < len(file_list):\n",
    "        print(\"  Table:\", file_list[i])\n",
    "        print(all_data[file_list[i]].dtypes, \"\\n\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write selected dataframe to default SQLite database, replacing if found\n",
    "def DataFrameToSQL(df, df_name):\n",
    "    connection = lite.connect(SQLITE_DB)\n",
    "    with connection:\n",
    "        df.to_sql(df_name, connection, schema = None, if_exists='replace', index=False)\n",
    "        print('%s backed up to default SQLite database.' % df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve SQL table from default SQLite database and store as a dataframe\n",
    "def SQLToDataFrame(table_name, date_columns=[]):\n",
    "    connection = lite.connect(SQLITE_DB)\n",
    "    sql = 'select * from ' + table_name\n",
    "    with connection:\n",
    "        df = pd.read_sql(sql, connection, parse_dates=date_columns)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of input file names \n",
    "file_list = ['admission', 'dx', 'flowsheet', 'lab', 'patient', 'service_team', 'rx','registry']\n",
    "\n",
    "## Dict of input files' date columns (by index) to convert \n",
    "date_columns = {'admission': [2,3],  'dx': [2], 'flowsheet': [4], \n",
    "                'lab': [4, 5], 'patient': [7, 8], 'service_team': [], 'rx': [3],'registry' : [1]}\n",
    "\n",
    "## Initialize empty OrderedDict to contain working dataframes. \n",
    "all_data = collections.OrderedDict()\n",
    "\n",
    "## Read tsv files into Pandas dataframes, and store in dict\n",
    "### Note: parse_dates is quite slow compared to pd.to_datetime\n",
    "for file in file_list:\n",
    "    file_path = DATA_PATH + file + '.txt'\n",
    "    df = pd.read_csv(file_path, sep = '\\t', header = 0, thousands = ',',\n",
    "                     parse_dates = date_columns[file], infer_datetime_format=True)\n",
    "    all_data[file] = df\n",
    "    print(file, df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ColumnSpaceToUnderscore(all_data, file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CheckDataTypes(all_data, file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store tables in temporary SQLite db\n",
    "## Initialize SQLite database for temporary storage\n",
    "connection = lite.connect(SQLITE_DB)\n",
    "\n",
    "### /with/ will automatically close connection when complete\n",
    "\n",
    "with connection:\n",
    "    i = 0\n",
    "    while i < len(file_list):\n",
    "        all_data[file_list[i]].to_sql(file_list[i], \n",
    "                                      connection, \n",
    "                                      schema = None, \n",
    "                                      if_exists='replace',\n",
    "                                      index=False)\n",
    "        print(file_list[i], 'table backed up to SQLite.')\n",
    "        i+=1\n",
    "        \n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distinct patients/encounters for each data set\n",
    "for each in file_list:\n",
    "    if each == 'flowsheet':\n",
    "        print(each, all_data[each].patientkey.nunique())\n",
    "    elif each == 'service_team':\n",
    "        print(each, all_data[each].encounterepiccsn.nunique())\n",
    "    elif each in ('admission'):\n",
    "        print(each, all_data[each].pat_enc_csn_id.nunique())\n",
    "   \n",
    "    else:\n",
    "        try:\n",
    "            print(each, all_data[each].patientkey.nunique())\n",
    "        except:\n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign new data frame names to all input data sets\n",
    "admission, dx, flowsheet, lab, patient, service_team, rx, registry = iter(all_data.values())\n",
    "del all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract date from admission for joining \n",
    "admission['adm_date'] = admission['hosp_admsn_time'].dt.date\n",
    "admission['adm_date'] = pd.to_datetime(admission['adm_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivots and Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: downsample dates with hierarchical index\n",
    "def DateGrouper(df, num_levels=3):\n",
    "    level_values = df.index.get_level_values\n",
    "    print(level_values)\n",
    "    # Downsampling method not defined; make sure to chain with .sum(), .last(), etc.\n",
    "    base_levels = list(range(0, num_levels-1))\n",
    "    return (df.groupby([level_values(i) for i in base_levels]\n",
    "                       +[pd.Grouper(freq=TIME_BINS, level=-1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: pivot long results into wide format\n",
    "### Inputs: dataframe, hierarchical index as list, date column, field name column, field value column, TIME_BINS\n",
    "### Output: pivoted dataframe\n",
    "def LongToWide(df, index_fields, date_field, name_field, value_field):\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    full_index = index_fields\n",
    "    full_index.append(name_field)\n",
    "    index_fields.append(date_field)\n",
    "    #df_temp.set_index(full_index, inplace=True, drop=False)\n",
    "    \n",
    "    ## Replace spaces in name_field with underscores\n",
    "    df_temp[name_field].replace(regex=True, inplace=True, to_replace=r'[ ]', value=r'_')\n",
    "    ## Remove commas in name_field \n",
    "    df_temp[name_field].replace(regex=True, inplace=True, to_replace=r'[,]', value=r'')\n",
    "    \n",
    "    #print(index_fields)\n",
    "    \n",
    "     ## Use DateGrouper to resample to TIME_BINS global var, using last observation\n",
    "    #df_temp = DateGrouper(df_temp, num_levels=len(index_fields)).last()\n",
    "\n",
    "    index_fields.remove(name_field)\n",
    "    \n",
    "    df_pivot = pd.pivot_table(df_temp, \n",
    "                             index=index_fields,\n",
    "                             values = value_field,\n",
    "                             columns = name_field,\n",
    "                             aggfunc = max)#np.max)#np.mean)#np.nanmax) \n",
    "    \n",
    "    ## Use DateGrouper to resample to TIME_BINS global var, using last observation\n",
    "    #df_pivot = DateGrouper(df_pivot, num_levels=len(index_fields)).last()\n",
    "    \n",
    "    df_pivot.reset_index(inplace=True)\n",
    "        \n",
    "    # Convert all columns to lower-case\n",
    "    df_pivot.rename(columns=str.lower, inplace=True)\n",
    "    \n",
    "    return(df_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Remove rows with missing measure values\n",
    "flow_na_removed = flowsheet.dropna(axis=0, how='any', subset=['flowvalue'], inplace=False)\n",
    "\n",
    "## Replace spaces in flowname with underscores\n",
    "flow_na_removed['flowname'].replace(regex=True, inplace=True, to_replace=r'[ ]', value=r'_')\n",
    "\n",
    "## Remove commas in flowname \n",
    "flow_na_removed['flowname'].replace(regex=True, inplace=True, to_replace=r'[,]', value=r'')\n",
    "\n",
    "flow_numerical_columns = ['PULSE_OXIMETRY', 'TEMPERATURE', 'PULSE',\n",
    "                          'BLOOD_PRESSURE', 'RESPIRATIONS', 'PKMOD_R_CPN_GLASGOW_COMA_SCALE_SCORE']\n",
    "\n",
    "flow_cat_columns = []\n",
    "\n",
    "flow_all_columns = flow_numerical_columns + flow_cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "flow_pivot = LongToWide(df=flow_na_removed[flow_na_removed.flowname.isin(flow_all_columns)], \n",
    "                        index_fields=['patientkey', 'encounterkey'], \n",
    "                        date_field='flowdatetime', \n",
    "                        name_field='flowname', \n",
    "                        value_field='flowvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select blood pressure measures, convert to strings, split on \"/\" character, convert to dictionary\n",
    "flow_pivot = flow_pivot.set_index(['patientkey','encounterkey','flowdatetime'])\n",
    "bp_split = flow_pivot.blood_pressure.str.split('/').to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from blood pressure dict, transpose, rename columns\n",
    "bp_df = pd.DataFrame.from_dict(bp_split, orient='columns').T.rename(columns = {0: 'systolic', 1: 'diastolic'}) \n",
    "\n",
    "# Merge bp dataframe back into pivoted flowsheet dataframe\n",
    "flow_temp = flow_pivot.reset_index().join(bp_df, how=\"left\", on=['patientkey','encounterkey','flowdatetime'])#pd.merge(flow_pivot2, bp_df, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "flow_pivot = flow_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn.matrix(flow_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = lab.replace({'labcommonname': {'POC HEMOGLOBIN': 'HEMOGLOBIN', 'GLUCOSE RANDOM': 'GLUCOSE',\\\n",
    "                                       'GLUCOSE UA': 'GLUCOSE', 'GLUCOSEPOC': 'GLUCOSE', 'POC POTASSIUM': 'POTASSIUM'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labs_pivot = LongToWide(df=lab, \n",
    "                        index_fields=['patientkey', 'encounterkey'], \n",
    "                        date_field='labcollectiontime', \n",
    "                        name_field='labcommonname', \n",
    "                        value_field='labvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labs_flag_pivot_base = LongToWide(df=lab, \n",
    "                                  index_fields=['patientkey', 'encounterkey'], \n",
    "                                  date_field='labcollectiontime', \n",
    "                                  name_field='labcommonname', \n",
    "                                  value_field='labflag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add _flag suffix to all columns except index columns.\n",
    "labs_flag_pivot = labs_flag_pivot_base.drop(['patientkey', 'encounterkey', 'labcollectiontime'],axis=1)\\\n",
    "                                      .add_suffix('_flag')\\\n",
    "                                      .join(labs_flag_pivot_base[['patientkey','encounterkey','labcollectiontime']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join lab values and lab flag pivots on patient, encounter, and time\n",
    "## OFF due to no flags in physiologic model\n",
    "#labs_pivot = pd.merge(labs_pivot, labs_flag_pivot, on=['patientkey','encounterkey','labcollectiontime'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outer join labs and flowsheets on patient, encounter, and time\n",
    "labs_flow_pivot = pd.merge(labs_pivot, flow_pivot, left_on=['patientkey','encounterkey','labcollectiontime'], \n",
    "                           right_on=['patientkey','encounterkey','flowdatetime'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge different versions of patient and encounter keys to reduce confusion\n",
    "labs_flow_pivot['patientkey'] = labs_flow_pivot['patientkey'].fillna(labs_flow_pivot.patientkey)\n",
    "labs_flow_pivot['encounterkey'] = labs_flow_pivot['encounterkey'].fillna(labs_flow_pivot.encounterkey)\n",
    "#labs_flow_pivot.drop(['patientkey','encounterkey'], axis=1, inplace=True)\n",
    "labs_flow_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrameToSQL(labs_flow_pivot, 'labs_flow_pivot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_flow_pivot = SQLToDataFrame(table_name='labs_flow_pivot', date_columns=['labcollectiontime', 'flowdatetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert analgesic dosages to morphine-equivalent-doses\n",
    "#rx['dose_mde'] = rx.conversion_to_oral_morphine * rx.rx_dose_min\n",
    "\n",
    "## Re-index and select columns to include\n",
    "rx = rx[['patientkey','encounterepiccsn', 'encounterkey', 'rxadministrationinstant', 'rxdosemin', 'rxshortname', 'rxroutetype']]\n",
    "#rx.set_index(['patient_key', 'encounter_key', 'rx_administration_instant'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_flow_pivot['current_datetime'] = labs_flow_pivot['flowdatetime'].fillna(labs_flow_pivot['labcollectiontime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_rx_flow = pd.merge(rx, labs_flow_pivot,\n",
    "                         left_on=['patientkey', 'encounterkey', 'rxadministrationinstant'],\n",
    "                         right_on=['patientkey', 'encounterkey', 'current_datetime'],\n",
    "                         how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create coalesced time elapsed since first measurement\n",
    "labs_flow_pivot2 = merge_rx_flow.copy()\n",
    "labs_flow_pivot2['current_datetime'] = labs_flow_pivot2['flowdatetime'].fillna(labs_flow_pivot2['labcollectiontime']).fillna(merge_rx_flow['rxadministrationinstant'])\n",
    "\n",
    "labs_flow_pivot2 = labs_flow_pivot2.join(labs_flow_pivot2.groupby(['patientkey','encounterkey']).current_datetime.min(), on=['patientkey','encounterkey'], rsuffix='_min')\n",
    "labs_flow_pivot2['time_elapsed'] = (labs_flow_pivot2.current_datetime - labs_flow_pivot2.current_datetime_min)#.astype('m8[h]')\n",
    "#labs_flow_pivot2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_flow_pivot2.drop(['labcollectiontime', 'rxadministrationinstant', 'flowdatetime', 'blood_pressure'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rx_flow_labs = labs_flow_pivot2.copy().set_index(['patientkey','encounterkey','time_elapsed'], drop=True)\n",
    "rx_sum = DateGrouper(rx_flow_labs[['rxdosemin']].copy()).sum().fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_sum['time_elapsed'] = rx_sum['time_elapsed'].astype('timedelta64[h]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_sum['time_sequence'] = rx_sum.time_elapsed.astype('timedelta64[h]') / np.timedelta64(12, 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrameToSQL(rx_sum, 'rx_sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_sum = SQLToDataFrame(table_name='rx_sum')\n",
    "# rx_sum['time_elapsed'] = rx_sum.time_sequence * np.timedelta64(12, 'h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max/Min/Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_vars = ['albumin', 'alt','ast', 'base_exc_art', 'bilirubin_total', 'co2', 'creatinine', 'crp', 'glucosepoc', 'glucose_random', \\\n",
    "#              'glucose_ua', 'hco3_art', 'hco3_ven', 'inr', 'lactate', 'lactate_bld_arterial', 'platelets','poc_inr', 'prealbumin',\\\n",
    "#              'sed_rate', 'wbc', 'pulse', 'pulse_oximetry','respirations', 'temperature', 'systolic', 'diastolic']\n",
    "\n",
    "# labs = ['albumin', 'alt', 'ast', 'base_exc_art', 'bilirubin_total', 'co2', 'creatinine', 'crp', 'glucosepoc', 'glucose_random',\\\n",
    "#         'glucose_ua', 'hco3_art', 'hco3_ven', 'inr', 'lactate', 'lactate_bld_arterial', 'platelets', 'poc_inr', 'prealbumin', \\\n",
    "#         'sed_rate', 'wbc']\n",
    "\n",
    "# vitals = list(set(time_vars) - set(labs))\n",
    "\n",
    "labs = ['platelets', 'wbc', 'lactate', 'lactate_bld_arterial', 'base_exc_art', 'albumin', 'prealbumin', 'sed_rate', 'glucose', \\\n",
    "        'hco3_ven', 'hco3_art', 'inr', 'poc_inr', 'creatinine', 'bilirubin_total', 'ast', 'alt', 'crp', 'co2', 'potassium', 'hemoglobin']\n",
    "vitals = ['temperature', 'pulse', 'pulse_oximetry', 'systolic']\n",
    "time_vars = labs + vitals\n",
    "\n",
    "max_vars = ['temperature', 'pulse', 'wbc', 'lactate', 'inr', 'creatinine', 'ast', 'alt', 'bilirubin_total', 'systolic', 'potassium']\n",
    "min_vars = ['temperature', 'pulse', 'pulse_oximetry', 'platelets', 'base_exc_art', 'albumin', 'systolic', 'potassium', 'hemoglobin']\n",
    "avg_vars = ['pulse', 'pulse_oximetry', 'systolic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Recode all labs and vitals as numeric\n",
    "# for col in time_vars:\n",
    "#     ## Use regex to search for symbols, text, and spaces; replace them with empty string; convert to numeric\n",
    "#     try:\n",
    "#         rx_flow_labs[col] = pd.to_numeric(re.sub(r\"[<>A-z ]\", \"\", rx_flow_labs[col]),errors='coerce')\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "## Recode all labs and vitals as numeric\n",
    "for col in time_vars:\n",
    "    ## Use regex to search for symbols, text, and spaces; replace them with empty string; convert to numeric\n",
    "    if col in rx_flow_labs.columns.values:\n",
    "        try:\n",
    "            rx_flow_labs[col] = pd.to_numeric(rx_flow_labs[col].astype(str).str.replace(r\"[<>A-z = ,]\", \"\"))\n",
    "        except TypeError:\n",
    "            print(col, 'could not be converted!')\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "flow_labs_to_fill = rx_flow_labs.copy().drop(['rxdosemin'],axis=1) #, 'rx_short_name', 'rx_route_type'\n",
    "flow_labs_filled = flow_labs_to_fill.copy().groupby(level=[0,1]).fillna(method='ffill', downcast='infer').groupby(level=[0,1]).fillna(method='bfill', downcast='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "flow_labs_last = DateGrouper(flow_labs_filled).last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "timevars_max = DateGrouper(flow_labs_filled.copy()[max_vars]).max()\n",
    "timevars_max = timevars_max.add_suffix('_max')\n",
    "\n",
    "timevars_min = DateGrouper(flow_labs_filled.copy()[min_vars]).min()\n",
    "timevars_min = timevars_min.add_suffix('_min')\n",
    "\n",
    "timevars_avg = DateGrouper(flow_labs_filled.copy()[avg_vars]).mean()\n",
    "timevars_avg = timevars_avg.add_suffix('_avg')\n",
    "\n",
    "\n",
    "## Calculate within-period max-min difference\n",
    "#itals_max = DateGrouper(flow_labs_filled.copy()[vitals]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# vitals_min = DateGrouper(flow_labs_filled.copy()[vitals]).min()\n",
    "# vitals_diff = vitals_max.astype(float) - vitals_min.astype(float)\n",
    "# vitals_diff = vitals_diff.add_suffix('_diff')\n",
    "# #vitals_diff.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_sum['time_elapsed'] = rx_sum.time_sequence * np.timedelta64(12, 'h')\n",
    "rx_sum.set_index(keys=['patientkey', 'encounterkey', 'time_elapsed'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_flow_labs = pd.merge(rx_sum, flow_labs_last, left_index=True, right_index=True, how='outer')\n",
    "rx_flow_labs = pd.merge(rx_flow_labs, timevars_max, left_index=True, right_index=True, how='outer')\n",
    "rx_flow_labs = pd.merge(rx_flow_labs, timevars_min, left_index=True, right_index=True, how='outer')\n",
    "rx_flow_labs = pd.merge(rx_flow_labs, timevars_avg, left_index=True, right_index=True, how='outer')\n",
    "rx_flow_labs.sort_index(level=1).head()\n",
    "\n",
    "# rx_flow_labs = pd.merge(rx_sum, flow_labs_last, left_index=True, right_index=True, how='outer')\n",
    "# rx_flow_labs = pd.merge(rx_flow_labs, vitals_diff, left_index=True, right_index=True, how='left')\n",
    "# rx_flow_labs.sort_index(level=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort data ascending by nullity\n",
    "sorted_data = mn.nullity_sort(rx_flow_labs, sort='ascending')\n",
    "mn.bar(sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_flow_labs2 = rx_flow_labs.copy().reset_index()\n",
    "\n",
    "## SQLite mangles timedelta fields, so create a durable time_sequence that is just a counter for observation windows\n",
    "rx_flow_labs2['time_sequence'] = rx_flow_labs2.time_elapsed / np.timedelta64(12, 'h')\n",
    "DataFrameToSQL(rx_flow_labs2.drop('time_elapsed',axis=1), 'rx_flow_labs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_flow_labs = SQLToDataFrame(table_name='rx_flow_labs', date_columns=['current_datetime', 'current_datetime_min'])\n",
    "\n",
    "# ## Reconvert durable time sequence into time elapsed (timedelta)\n",
    "# rx_flow_labs['time_elapsed'] = rx_flow_labs.time_sequence * np.timedelta64(12, 'h')\n",
    "# rx_flow_labs.set_index(['patientkey', 'encounterkey', 'time_elapsed'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DataFrameToSQL1(df, df_name):\n",
    "#     connection = lite.connect(DATA_PATH+'trauma_tast_merge_temp1.db')\n",
    "#     with connection:\n",
    "#         df.to_sql(df_name, connection, schema = None, if_exists='replace', index=False)\n",
    "#         print('%s backed up to default SQLite database.' % df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameToSQL1(merge_temp1,'merge_temp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Left join ADMISSION with RX_FLOW_LABS\n",
    "merge_temp1 = pd.merge(admission,rx_flow_labs.reset_index(), left_on = ['pat_enc_csn_id'], right_on = 'encounterepiccsn', how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Left join merge_temp1 and patient on patientkey\n",
    "merge_temp1 = pd.merge(merge_temp1, patient, \n",
    "                                   left_on='patientkey', \n",
    "                                   right_on='patientkey', \n",
    "                                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_team_sicu = service_team[service_team['serviceteamname'].isin(['SICU TEAM', 'NCC TEAM', 'NEURO CRITICAL CARE'])]\n",
    "service_team_sicu = service_team_sicu.drop_duplicates(keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join merge_temp1 and service_team where service team name in SICU team, NCC team, Neuro Critical care team\n",
    "merge_temp1 = pd.merge(merge_temp1,service_team_sicu, left_on='pat_enc_csn_id', right_on = 'encounterepiccsn',how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove registry rows with invalid identifers\n",
    "registry_clean = registry[registry.mrn!='0 0 0 0 0 0 0 0']\n",
    "registry_clean = registry_clean[registry_clean.mrn!='                    '] \n",
    "\n",
    "## Remove registry rows with missing identifiers\n",
    "registry_clean = registry_clean[pd.isnull(registry_clean.mrn)==False]\n",
    "\n",
    "## Remove invalid character from registry identifier field\n",
    "registry_clean.mrn.replace(regex=True, inplace=True, to_replace=r'[`]', value='')\n",
    "\n",
    "## Convert registry identifier to float\n",
    "registry_clean.mrn = registry_clean.mrn.astype(float)\n",
    "\n",
    "## Remove invalid characters from ISS variable\n",
    "registry_clean.iss = registry_clean.iss[~registry_clean.iss.isin(['   ','UA'])]\n",
    "\n",
    "## Convert date of arrival to datetime\n",
    "registry_clean['Date of Arrival'] = pd.to_datetime(registry_clean['date_of_arrival'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Left join merge_temp1 and registry_clean on ['mrn', 'date_of_arrival']\n",
    "merge_temp1 = pd.merge(merge_temp1, registry_clean, \n",
    "                                   left_on=['primarymrn', 'adm_date'],\n",
    "                                   right_on=['mrn', 'date_of_arrival'], \n",
    "                                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a sicu flag in case we need to subset only SICU patients from the cohort\n",
    "merge_temp1['sicu_flag'] = 0\n",
    "for team in ['SICU TEAM', 'NCC TEAM', 'NEURO CRITICAL CARE']:\n",
    "    #df.ix[(df['A'] == df['B']), 'C'] = 0\n",
    "    merge_temp1.loc[merge_temp1['serviceteamname']==team,'sicu_flag'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Left join merge_temp1 and dx on ['patientkey', 'encounterkey']\n",
    "merge_temp1_dx =  pd.merge(merge_temp1, dx, \n",
    "                    on=['patientkey', 'encounterkey'],\n",
    "                     how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(merge_temp1_dx.shape)\n",
    "no_burn_id = merge_temp1_dx[merge_temp1_dx.dxname.str.contains('burn|Burn')==False]['pat_enc_csn_id'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Including only non-burn patients\n",
    "merge_temp1_dx = merge_temp1_dx[merge_temp1_dx.pat_enc_csn_id.isin(no_burn_id)].sort_values('pat_enc_csn_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_temp1 =  merge_temp1_dx.drop(['dxstartdate', 'hospitaldiagnosis',\n",
    "       'emergencydepartmentdiagnosis', 'dxname', 'dxdisplayname', 'dxtype',\n",
    "       'dxstatus', 'dxpresentonadmission', 'dxcodetype', 'dxcode'], axis = 1).drop_duplicates(keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting few dataframes to combat memory error\n",
    "del labs_flow_pivot\n",
    "del rx_sum\n",
    "del rx_flow_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrameToSQL(merge_temp1_dx,'merge_temp1_dx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrameToSQL(merge_temp1, 'merge_temp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all = merge_temp1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all['time_elapsed'] = merge_all.time_sequence * np.timedelta64(12, 'h')\n",
    "merge_all['time_elapsed'].fillna(0, inplace=True)\n",
    "merge_all['time_sequence'] = merge_all.time_elapsed / np.timedelta64(12, 'h')\n",
    "merge_all['hours_elapsed'] = merge_all.time_elapsed / np.timedelta64(1, 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop unneeded or highly missing columns\n",
    "try:\n",
    "    merge_all = merge_all.drop(['department_name', 'encounterepiccsn_x','encounterepiccsn_y', 'religion',\n",
    "                            'hsp_account_id', 'patientkey', 'encounterkey'], axis=1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate BMI\n",
    "#merge_all['bmi'] = ((merge_all['weight_lb'].astype(float)) / merge_all['height_in'].astype(float)**2) * 703\n",
    "## Replace BMI's beyond 7:100 with null, to be imputed later due to bad weight/height measurements\n",
    "#merge_all['bmi'] = merge_all.bmi.where(merge_all.bmi>7, None).where(merge_all.bmi < 100, None).astype(float)\n",
    "\n",
    "## Calculate age in years\n",
    "merge_all['age'] = (merge_all.hosp_admsn_time - merge_all.birthdate).astype('<m8[Y]') \n",
    "\n",
    "## Create death flags for potential dependent variables\n",
    "merge_all['death_flag_ever'] = abs((merge_all['deathdate'].isnull()).astype(float)-1)\n",
    "merge_all['death_flag_inhospital'] = (merge_all['deathdate'] <= merge_all['hosp_dischrg_time']).astype(float)\n",
    "merge_all['death_flag_registry'] = pd.get_dummies(merge_all['death_case'])['Yes']\n",
    "merge_all['death_flag_this_period'] = ((merge_all['time_elapsed'] == merge_all.groupby(['primarymrn','pat_enc_csn_id'])['time_elapsed'].transform(max))\\\n",
    "              & (merge_all.death_flag_inhospital == 1)).astype(float)\n",
    "merge_all['death_flag_next_period'] = (((merge_all['time_sequence'] == merge_all.groupby(['primarymrn','pat_enc_csn_id'])['time_sequence'].transform(max)-1)\\\n",
    "                                        | (merge_all.groupby(['primarymrn','pat_enc_csn_id'])['time_sequence'].transform(max) == 0))\\\n",
    "                                        & (merge_all.death_flag_inhospital == 1)).astype(float)\n",
    "merge_all['death_flag_two_periods'] = ((merge_all['time_sequence'] == merge_all.groupby(['primarymrn','pat_enc_csn_id'])['time_sequence'].transform(max)-2)\\\n",
    "                                        & (merge_all.death_flag_inhospital == 1)).astype(float)\n",
    "merge_all['death_flag_three_periods'] = ((merge_all['time_sequence'] == merge_all.groupby(['primarymrn','pat_enc_csn_id'])['time_sequence'].transform(max)-3)\\\n",
    "                                        & (merge_all.death_flag_inhospital == 1)).astype(float)\n",
    "merge_all['death_flag_four_periods'] = ((merge_all['time_sequence'] == merge_all.groupby(['primarymrn','pat_enc_csn_id'])['time_sequence'].transform(max)-4)\\\n",
    "                                        & (merge_all.death_flag_inhospital == 1)).astype(float)\n",
    "#merge_all[merge_all.death_flag_inhospital==1][['pat_enc_csn_id','current_datetime', 'time_elapsed', 'death_flag_inhospital','death_flag_this_period']].sort_values(['pat_enc_csn_id','time_elapsed']).head(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace missing dose_mde with 0\n",
    "merge_all.rxdosemin.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Set multi-level index\n",
    "merge_all = merge_all.set_index(['primarymrn', 'pat_enc_csn_id', 'time_sequence'])\n",
    "\n",
    "## Remove duplicates by multi-index\n",
    "merge_all = merge_all[~merge_all.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vars = ['albumin', 'alt', 'ast', 'base_exc_art', 'bilirubin_total', 'co2',\n",
    "       'creatinine', 'crp', 'glucose', 'hco3_art', 'hco3_ven',\n",
    "       'hemoglobin', 'inr', 'lactate', 'lactate_bld_arterial',\n",
    "       'platelets', 'poc_inr', 'potassium', 'prealbumin', 'sed_rate',\n",
    "       'wbc', 'pkmod_r_cpn_glasgow_coma_scale_score', 'pulse',\n",
    "       'pulse_oximetry', 'respirations', 'temperature', 'systolic',\n",
    "       'diastolic', 'temperature_max', 'pulse_max', 'wbc_max', 'lactate_max',\n",
    "       'inr_max', 'creatinine_max', 'ast_max', 'alt_max',\n",
    "       'bilirubin_total_max', 'systolic_max', 'potassium_max',\n",
    "       'temperature_min', 'pulse_min', 'pulse_oximetry_min',\n",
    "       'platelets_min', 'base_exc_art_min', 'albumin_min', 'systolic_min',\n",
    "       'potassium_min', 'hemoglobin_min', 'pulse_avg',\n",
    "       'pulse_oximetry_avg', 'systolic_avg']\n",
    "\n",
    "non_time = list(set(merge_all.columns.values) - set(time_vars) - set(list(merge_all.columns[merge_all.columns.str.endswith('flag')])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Fill forward (non-time forever and time-based for 2 periods) and backward (non-time-based variables only) to close gaps in between data points -- slow!\n",
    "## Limit = 2,  otherwise, results in clinical inconsistencies between similar measures (e.g. lactate and base excess)\n",
    "merge_all[time_vars] = merge_all[time_vars].sort_index().groupby(level=[0,1]).fillna(method='ffill', downcast='infer', limit=2)\n",
    "merge_all[non_time] = merge_all[non_time].sort_index().groupby(level=[0,1]).fillna(method='ffill', downcast='infer')\n",
    "merge_all[non_time] = merge_all[non_time].sort_index().groupby(level=[0,1]).fillna(method='bfill', downcast='infer')\n",
    "\n",
    "# ## Fill forward (all) and backward (non-time-based variables only) to close gaps in between data points -- slow!\n",
    "# merge_all = merge_all.sort_index().groupby(level=[0,1]).fillna(method='ffill', downcast='infer')\n",
    "# merge_all[non_time] = merge_all[non_time].sort_index().groupby(level=[0,1]).fillna(method='bfill', downcast='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameToSQL(merge_all, 'merge_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Left join full data set and transfusions on index\n",
    "# merge_t = pd.merge(merge_all.copy().reset_index(), transfusion_, left_on=['pat_enc_csn_id', 'time_sequence'], right_on=['pat_enc_csn_id', 'time_sequence'], how='left')\n",
    "# merge_t[include_columns] = merge_t[include_columns].fillna(0)\n",
    "# merge_all = merge_t.set_index(['primary_mrn', 'pat_enc_csn_id', 'time_sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate shock index, mean blood pressure (map), pmi, bpai, etc.\n",
    "merge_all.pulse = merge_all.pulse.astype(float)\n",
    "merge_all.systolic = merge_all.systolic.astype(float)\n",
    "merge_all.diastolic = merge_all.diastolic.astype(float)\n",
    "\n",
    "merge_all['map'] = ((merge_all.diastolic * 2 + merge_all.systolic) / 3)\n",
    "merge_all['minpulse'] = (220 - merge_all.age) - merge_all.pulse\n",
    "merge_all['pulse_max_index'] = merge_all.pulse / (220 - merge_all.age)\n",
    "merge_all['blood_pressure_age_index'] = merge_all.systolic / merge_all.age\n",
    "\n",
    "merge_all['shock_index'] = merge_all.pulse / merge_all.systolic\n",
    "merge_all['shock_index_modified'] = merge_all.pulse / merge_all.map\n",
    "merge_all['shock_index_age'] = merge_all.shock_index * merge_all.age\n",
    "merge_all['shock_index_reverse'] = merge_all.systolic / merge_all.pulse\n",
    "merge_all['shock_index_reverse_lt1'] = (merge_all['shock_index_reverse'] < 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Year, month, and weekday of arrival\n",
    "merge_all['arrival_year'] = merge_all.hosp_admsn_time.dt.year\n",
    "merge_all['arrival_month'] = merge_all.hosp_admsn_time.dt.month\n",
    "merge_all['arrival_weekday'] = merge_all.hosp_admsn_time.dt.dayofweek\n",
    "\n",
    "#merge_all['current_year'] = merge_all.current_datetime.dt.year\n",
    "#merge_all['current_month'] = merge_all.current_datetime.dt.month\n",
    "#merge_all['current_weekday'] = merge_all.current_datetime.dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regroup language, race to include only values with more than 200 patients\n",
    "merge_all.loc[~merge_all.preferredlanguage.isin(['English','Spanish']), 'preferredlanguage'] = 'Other'\n",
    "merge_all.loc[~merge_all.firstrace.isin(['White', 'Black', 'Asian']), 'firstrace'] = 'Other'\n",
    "\n",
    "## Remove invalid ISS observations\n",
    "merge_all.loc[merge_all.iss.isin(['UA', '', '   ']), 'iss'] = None\n",
    "merge_all.iss = merge_all.iss.astype(float)\n",
    "\n",
    "## Regroup smoking_status into bigger categories\n",
    "merge_all.loc[merge_all.smokingstatus.isin(['Current Every Day Smoker', 'Current Some Day Smoker', 'Smoker, Current Status Unknown', 'Heavy Tobacco Smoker', 'Light Tobacco Smoker']), 'smokingstatus'] = 'Current Smoker'\n",
    "merge_all.loc[merge_all.smokingstatus.isin(['Never Smoker ', 'Passive Smoke Exposure - Never Smoker']), 'smokingstatus'] = 'Never Smoker'\n",
    "merge_all.loc[~merge_all.smokingstatus.isin(['Current Smoker', 'Never Smoker', 'Former Smoker']), 'smokingstatus'] = 'Unknown'\n",
    "\n",
    "## Regroup marital status values\n",
    "merge_all.loc[merge_all.maritalstatus.isin(['Common Law', 'Significant']), 'maritalstatus'] = 'Married'\n",
    "merge_all.loc[merge_all.maritalstatus.isin(['Unknown', '*Unspecified']), 'maritalstatus'] = 'Other'\n",
    "\n",
    "## Regroup specialty to include only values with more than 200 patients\n",
    "merge_all.loc[merge_all.specialty.isin(['RADIOLOGY','GERIATRICS','GENERAL INTERNAL MEDICINE',\n",
    "                                        '2SS Observation','OBSTETRICS','PULMONARY','DIABETES',\n",
    "                                        'INTERNAL MEDICINE','ONCOLOGY','PSYCHIATRY','OCCUPATIONAL MEDICINE',\n",
    "                                        'PHYSICAL THERAPY','GYNECOLOGY','DERMATOLOGY',\n",
    "                                        'PHYSICAL MEDICINE & REHAB' ]), 'specialty'] = 'OTHER'\n",
    "\n",
    "## Regroup financial class categories\n",
    "merge_all.loc[merge_all.primaryfinancialclass.isin(['Tricare', 'Champva', 'Worker\\'s Comp']), 'primaryfinancialclass'] = 'Other'\n",
    "merge_all.loc[merge_all.primaryfinancialclass == 'Blue Shield', 'primaryfinancialclass'] = 'Commercial'\n",
    "\n",
    "## Regroup mechanism of injury (MOI)\n",
    "merge_all.moi.fillna('', inplace=True)\n",
    "merge_all.loc[merge_all['moi'].str.lower().str.contains('gun') | (merge_all['moi'] == 'GSW'), 'moi'] = 'gsw'\n",
    "merge_all.loc[merge_all['moi'].str.lower().str.contains('fall'), 'moi'] = 'fall'\n",
    "merge_all.loc[merge_all['moi'].str.lower().str.contains('mva') | (merge_all['moi'] == 'MVC'), 'moi'] = 'mvc'\n",
    "merge_all.loc[merge_all['moi'].str.lower().str.contains('mtrcycle'), 'moi'] = 'mcc'\n",
    "merge_all.loc[merge_all['moi'].isin(['Not Applicable                          ', 'Other', '                                        ']),'moi'] = 'other'\n",
    "merge_all.loc[merge_all['moi'] == 'MPC', 'moi'] = 'mpc'\n",
    "for x in ['cut', 'stab']: merge_all.loc[merge_all['moi'].str.lower().str.contains(x), 'moi'] = 'sharp'\n",
    "for x in ['burn', 'hot']: merge_all.loc[merge_all['moi'].str.lower().str.contains(x), 'moi'] = 'burn'\n",
    "for x in ['assault', 'struck', 'fight']: merge_all.loc[merge_all['moi'].str.lower().str.contains(x), 'moi'] = 'assault'\n",
    "merge_all.loc[~merge_all['moi'].isin(['fall','mvc','burn','sharp','assault','mcc','gsw','mpc']),'moi'] = 'other'\n",
    "\n",
    "## Recode age as bins\n",
    "bins = [0, 15, 24, 44, 64, 120]\n",
    "group_names = ['<16', '16-24', '25-44', '45-64', '>=65']\n",
    "merge_all['age_bin'] = pd.cut(merge_all.age, bins = bins, labels = group_names)\n",
    "merge_all['age_lt16'] = pd.get_dummies(merge_all.age_bin)['<16']\n",
    "merge_all['age_16-24'] = pd.get_dummies(merge_all.age_bin)['16-24']\n",
    "merge_all['age_25-44'] = pd.get_dummies(merge_all.age_bin)['25-44']\n",
    "merge_all['age_45-64'] = pd.get_dummies(merge_all.age_bin)['45-64']\n",
    "merge_all['age_gt65'] = pd.get_dummies(merge_all.age_bin)['>=65']\n",
    "#Set all age columns to missing if missing in original\n",
    "merge_all.loc[merge_all.age_bin.isnull(), merge_all.columns.str.startswith(\"age_\")] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ## Recode sex, race, financial class\n",
    "    merge_all['female'] = pd.get_dummies(merge_all, columns=['sex'], drop_first=True)['sex_Female']\n",
    "    merge_all['hispanic'] = pd.get_dummies(merge_all['ethnicity'])['Hispanic']\n",
    "    \n",
    "    ## Drop recoded/replaced/missing variables\n",
    "    merge_all.drop(['sex', 'ethnicity', 'death_case', 'pkip_r_cam-icu_feature_1', 'pkip_r_cam-icu_feature_2',\n",
    "                    'pkip_r_cam-icu_feature_3', 'pkip_r_cam-icu_feature_4','pain_scale','pkmod_r_vent_mode'], \n",
    "                    axis=1, inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace any infinite values with NaN\n",
    "merge_all = merge_all.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recode all labs and vitals as numeric\n",
    "time_vars_all = time_vars + ['map', 'minpulse', 'pulse_max_index', 'blood_pressure_age_index', 'shock_index', 'shock_index_modified',\n",
    "              'shock_index_age', 'shock_index_reverse']\n",
    "\n",
    "for col in time_vars_all:\n",
    "    merge_all[col] = pd.to_numeric(merge_all[col].replace(to_replace=r'[<>A-z ]', value='', regex=True), errors='coerce', downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_plus = list(set(time_vars_all) - set(lab))\n",
    "\n",
    "# Difference by period offset\n",
    "df = merge_all.copy()[vitals_plus]\n",
    "df = df.sort_index().groupby(level=[0,1]).diff(periods=1, axis=0)\n",
    "df = df.add_suffix('_period_diff')\n",
    "merge_all = pd.merge(merge_all, df, left_index=True, right_index=True, how='left')\n",
    "merge_all[list(df.columns.values)] = merge_all[list(df.columns.values)].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluding burn patients per stakeholder inputs since burn patients can have varying results\n",
    "merge_all = merge_all[~merge_all.moi.isin(['burn'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrameToSQL(merge_all.copy().reset_index(), 'merge_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all = SQLToDataFrame(table_name='merge_all', date_columns=['date_of_arrival', 'current_datetime', 'current_datetime_min', 'birthdate', 'deathdate', 'hosp_admsn_time', 'hosp_dischrg_time'])\n",
    "# merge_all['time_elapsed'] = merge_all.time_sequence * np.timedelta64(12, 'h')\n",
    "# merge_all.set_index(['primary_mrn','pat_enc_csn_id','time_sequence'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pivot tables for demographics\n",
    "categoricals = ['moi', 'specialty',  'maritalstatus','smokingstatus', 'primaryfinancialclass', \n",
    "                'preferredlanguage', 'firstrace', 'arrival_year', 'arrival_month', 'arrival_weekday']\n",
    "                #'current_year', 'current_month', 'current_weekday']\n",
    "df = merge_all.copy().reset_index()\n",
    "df = df[~df.primarymrn.duplicated(keep='first')]\n",
    "df['i'] = 1\n",
    "\n",
    "for each in categoricals:\n",
    "    counts = pd.pivot_table(df, values='i', index=each, columns=['death_flag_inhospital'], aggfunc='count')\n",
    "    print(counts)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = merge_all.reset_index().set_index('pat_enc_csn_id')\n",
    "data = data[~data.index.duplicated(keep='last')]\n",
    "\n",
    "mn.bar(data[time_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = merge_all.corr()\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(df_corr, vmax=.5, square=True)\n",
    "plt.show()\n",
    "\n",
    "print(df_corr[['death_flag_this_period']].abs().sort_values(by='death_flag_this_period', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all.reset_index().groupby('death_flag_next_period').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rough deathrate by MRN\n",
    "print('Death rate by patient: %0.3f' % (merge_all.reset_index()[['pat_enc_csn_id','death_flag_next_period']].drop_duplicates().death_flag_next_period.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count unique patients by year\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 8\n",
    "t_cohort = merge_all.reset_index()[['date_of_arrival','primarymrn']].drop_duplicates()\n",
    "t_cohort = t_cohort.set_index('date_of_arrival')\n",
    "t_cohort = t_cohort.resample('12M').count()#.plot()\n",
    "ax = t_cohort.plot(kind='area', alpha=.7, title=\"Patient Count Over Time\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all.reset_index()[['primarymrn','date_of_arrival','death_flag_inhospital']].drop_duplicates().set_index('date_of_arrival')\\\n",
    ".resample('12M').mean().drop('primarymrn',axis=1).plot.area(stacked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"age_bin\", y=\"glucose_random\", hue=\"death_flag_inhospital\", data=merge_all, palette=\"PRGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
